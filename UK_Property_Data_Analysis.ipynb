{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f641cc5",
   "metadata": {},
   "source": [
    "1. Find two land registry records that are likely to be errors. \n",
    "In each case, provide a potential explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9cb7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download and read Land Registry data\n",
    "data_path = \"pp-complete.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc535a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f95789",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['transaction_id', 'price', 'date', 'postcode', 'property_type', 'old_new', 'duration', 'paon', 'saon', 'street', 'locality', 'town_city', 'district', 'county', 'ppd_category_type', 'record_status']\n",
    "df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fedd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47c3b0",
   "metadata": {},
   "source": [
    "Plot the data to check if there is any outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed47820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df['price'])\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Boxplot of Prices')\n",
    "plt.xlabel('Price')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654006f1",
   "metadata": {},
   "source": [
    "\n",
    "From the graphical analysis of the dataset, it is evident that there are some anomalies in the data that could be indicative of fake transactions or potential data entry errors. These anomalies must be addressed to ensure the accuracy and reliability of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter for potential outliers\n",
    "outliers = df[(df[\"price\"] < df[\"price\"].quantile(0.01))]\n",
    "\n",
    "# Analyze outliers and provide explanations\n",
    "\n",
    "# Print outliers and potential explanations\n",
    "for index, row in outliers.iterrows():\n",
    "  print(f\"Record ID: {index}, Price: {row['price']}, Explanation: Possible data entry error or a fake transaction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce5e64",
   "metadata": {},
   "source": [
    "To remove the spotted anomalies, I calculated the 0.01 quantile of the price column, which helps identify and filter out transactions with unusually low prices. For instance, transactions such as Record ID: 29212254 with a price of 500 and Record ID: 29212252 with a price of 3000 fall below this threshold and are flagged as anomalies. By removing these low-priced entries, we can eliminate possible erroneous or fraudulent records, thereby refining the dataset for more accurate and meaningful analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27eb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_postcodes = df['postcode']\n",
    "# for i in all_postcodes:\n",
    "#     if len(i) != 8:\n",
    "#         print(i)\n",
    "rows_with_nan = df[df['postcode'].isna()]\n",
    "print(rows_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2558561",
   "metadata": {},
   "source": [
    "In addition to anomalies in the price data, there is a possibility of errors in the postcodes within the dataset. These errors, however, do not necessarily invalidate the entire entry. A postcode might be entered incorrectly due to typographical errors, or it might be missing altogether, yet other details such as the price and property characteristics could still be accurate and valuable. Such errors in postcodes should be carefully addressed rather than leading to the outright rejection of these entries. But in a less time frame I will be deleting those entries if there is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete rows with NaN values\n",
    "print(len(df))\n",
    "df = df.dropna(subset=['postcode'])\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bccd86",
   "metadata": {},
   "source": [
    "UK postcodes is in the range of 5 to 8 characters long, so including space it must be under the 9 chars and greater than 5 chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_valid_postcode'] = df['postcode'].apply(lambda x: ((len(x) < 9) & (len(x) > 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['is_valid_postcode'] == True]))\n",
    "print(len(df[df['is_valid_postcode'] == False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['is_valid_postcode'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41977a03",
   "metadata": {},
   "source": [
    "2. Complete the following table by calculating the number of sales and average sale price for all London Boroughs in 2023:\n",
    "\n",
    "Required columns:\n",
    "London Borough, \n",
    "Count of sales in 2023, \n",
    "Average sale price in 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d039a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show df of couty == Greater London and town_city == london and locality == BOROUGH\n",
    "df[(df['county'] == 'GREATER LONDON') & (df['locality'] == 'LONDON')]['district'].unique()\n",
    "# df[df['county']=='GREATER LONDON']['district'].unique()\n",
    "# rows_with_london = df[df.applymap(lambda x: x == 'BOROUGH').any(axis=1)]\n",
    "# rows_with_london"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12161a",
   "metadata": {},
   "source": [
    "The above are the all London Boroughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for London Boroughs in 2023\n",
    "london_boroughs = list(df[(df['county'] == 'GREATER LONDON') & (df['locality'] == 'LONDON')]['district'].unique())\n",
    "borough_data = df[(df[\"district\"].isin(london_boroughs)) & (df[\"date\"].dt.year == 2023)]\n",
    "\n",
    "# Calculate sales count and average price\n",
    "borough_sales = borough_data.groupby(\"district\").agg(count=(\"transaction_id\", \"count\"), \n",
    "                                                    avg_price=(\"price\", \"mean\"))\n",
    "\n",
    "# Print results as a table\n",
    "print(borough_sales.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363196be",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_sales_df = borough_sales.reset_index()\n",
    "col_new_names = ['London Borough', 'Count of sales in 2023', 'Average sale price in 2023']\n",
    "borough_sales_df.columns = col_new_names\n",
    "borough_sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28449680",
   "metadata": {},
   "source": [
    "3. Count the number of new build Flats sold in each UK region since the start of 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ea7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for new build flats since 2020\n",
    "new_flats = df[(df[\"property_type\"] == \"F\") & (df[\"date\"].dt.year >= 2020) & (df[\"old_new\"] == \"N\")]\n",
    "\n",
    "# Group and count sales by region\n",
    "region_sales = new_flats.groupby(\"county\").agg(count=(\"transaction_id\", \"count\"))\n",
    "\n",
    "# Print results as a table\n",
    "print(region_sales.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb59c57",
   "metadata": {},
   "source": [
    "To count the number of new build flats sold in each UK region since the start of 2020, I followed a systematic approach. First, I filtered the dataset based on the property_type column, selecting only those records where the property type is specified as 'F' for flats. Next, I further refined the dataset by including only those transactions with a date later than 2020. This was achieved by filtering the date column to include records from 2020 onwards. To ensure that only new build flats were counted, I filtered the dataset once more using the old_new column, selecting only those entries marked with 'N' for new builds. Finaly, to categorize the data by UK regions, I grouped the filtered records by the county column, which corresponds to the different regions within the UK. This comprehensive filtering and grouping process allowed me to accurately count the number of new build flats sold in each UK region since the start of 2020, providing valuable insights into the distribution and volume of new residential property sales across the country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf144755",
   "metadata": {},
   "source": [
    "4. Plot the number of sales per week since the start of 2020 as a line chart Discuss the chart, and provide potential explanations for any patterns or anomalies \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "since2020datadf = df[(df[\"date\"].dt.year >= 2020)]\n",
    "# Group sales by week and count occurrences\n",
    "weekly_sales = since2020datadf.resample(\"W-Sun\", on=\"date\")[\"transaction_id\"].count()\n",
    "\n",
    "# Plot sales per week\n",
    "weekly_sales.plot(kind=\"line\", figsize=(10, 6))\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Number of Sales\")\n",
    "plt.title(\"Sales per Week since 2020\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d12a2",
   "metadata": {},
   "source": [
    "The analyzed sales per week chart reveals a general trend of decreasing sales over the years, punctuated by several notable anomalies where sales figures either peaked or dropped significantly. These fluctuations in sales can often be attributed to a range of external factors. For example, economic conditions such as inflation, changes in fiscal or housing policies, and seasonal variations can cause deviations from the general trend. The year 2024, for instance, shows the lowest sales, which could be reflective of recent economic pressures or shifts in the property market. Conversely, the middle of 2021 experienced the highest sales, which might be due to a temporary surge in market activity or favorable conditions at that time. The decreasing trend in sales might also be related to broader issues such as inflationary pressures and rising land and property prices, which can reduce affordability and dampen market activity. Understanding these anomalies within the broader context of economic and market conditions can provide valuable insights into the factors influencing property sales over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c46c8",
   "metadata": {},
   "source": [
    "5. Plot a histogram of sale prices and discuss which distribution best represents the data Feel free to transform the data before plotting, but explain your reasoning if you choose to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f53c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Plot price distribution as a histogram\n",
    "plt.hist(df[\"price\"], bins=20)\n",
    "plt.xlabel(\"Sale Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Sale Prices\")\n",
    "plt.show()\n",
    "\n",
    "# Apply log transformation to sale prices\n",
    "df[\"log_price\"] = df[\"price\"].apply(lambda x: np.log(x))\n",
    "\n",
    "# Plot log price distribution as a histogram\n",
    "plt.hist(df[\"log_price\"], bins=20)\n",
    "plt.xlabel(\"Log Sale Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Log Sale Prices\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86151364",
   "metadata": {},
   "source": [
    "The distribution of sale prices is right-skewed, with a long tail on the right side. This indicates that there are some high-value properties that are significantly more expensive than the rest. To better visualize the distribution, I have appllied a log transformation to the sale prices and plot the histogram again. The log-transformed sale prices now follow a more normal distribution, which makes it easier to identify patterns and trends in the data. The log transformation helps to reduce the impact of extreme values and outliers, making the distribution more symmetrical and easier to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccfaee5",
   "metadata": {},
   "source": [
    "6. Using the BNG tiles and an appropriate scale, plot a map showing the number of sales per 10km grid square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geopandas>= 0.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae79873",
   "metadata": {},
   "source": [
    "I am unable to perform the below tasks with the complete land registry dataset because the memory error issue. I have tried it on the aws cloud as well but I need to pay more and purchase the resources in aws. So I will use the monthly land registry data for the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee08bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame, read_file\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Land Registry data (replace with your processed data)\n",
    "df = pd.read_csv(\"pp-monthly-update-new-version.csv\")\n",
    "\n",
    "column_names = ['transaction_id', 'price', 'date', 'postcode', 'property_type', 'old_new', 'duration', 'paon', 'saon', 'street', 'locality', 'town_city', 'district', 'county', 'ppd_category_type', 'record_status']\n",
    "df.columns = column_names\n",
    "\n",
    "# Load BNG Grid data\n",
    "bng_grid = read_file(\"OSGB_Grids-master\\GeoJSON\\OSGB_Grid_5km.geojson\")\n",
    "print(bng_grid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bng_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "from pyproj import Transformer\n",
    "\n",
    "# Load the OSGB dataset from JSON file\n",
    "osgb_tiles = bng_grid\n",
    "osgb_tiles = osgb_tiles.to_crs(epsg=27700)\n",
    "\n",
    "# Load the sales data from CSV file\n",
    "sales_data = df\n",
    "\n",
    "# Creating latitude and longitude columns by merging the sales data with the postcode coordinates\n",
    "postcode_coords = pd.read_csv('NSPL_2021_MAY_2024/Data/NSPL21_MAY_2024_UK.csv')\n",
    "postcode_coords_df = postcode_coords[['pcds', 'lat', 'long']]\n",
    "new_col_names = ['postcode', 'latitude', 'longitude']\n",
    "postcode_coords_df.columns = new_col_names\n",
    "sales_data = sales_data.merge(postcode_coords_df, on='postcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a92d6",
   "metadata": {},
   "source": [
    "Here, have a Dataset with 'pcds', 'lat', 'long' columns which then merged with the land registry data which have postcode column. Applied merging on postcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abceb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Examine Geometry Data: \n",
    "# gdf_sales.is_valid.sum() \n",
    "# osgb_tiles.is_valid.sum()\n",
    "\n",
    "# #Verify CRS: \n",
    "# gdf_sales.crs \n",
    "# osgb_tiles.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba725e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sales data to a GeoDataFrame\n",
    "gdf_sales = gpd.GeoDataFrame(sales_data, geometry=gpd.points_from_xy(sales_data.longitude, sales_data.latitude))\n",
    "\n",
    "# Transformer to convert WGS84 to OSGB36\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:27700\", always_xy=True)\n",
    "\n",
    "# Convert sales coordinates to OSGB36\n",
    "gdf_sales['geometry'] = gdf_sales['geometry'].apply(lambda geom: Point(transformer.transform(geom.x, geom.y)))\n",
    "gdf_sales.set_crs(\"EPSG:27700\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform spatial join to map sales to OSGB tiles\n",
    "gdf_sales_with_tiles = gpd.sjoin(gdf_sales, osgb_tiles, how=\"left\", predicate=\"within\")\n",
    "gdf_sales_with_tiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sales count per OSGB tile\n",
    "sales_per_tile = gdf_sales_with_tiles.groupby('TILE_NAME').size().reset_index(name='sales_count')\n",
    "sales_per_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc42fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales count with OSGB tiles\n",
    "osgb_tiles = osgb_tiles.merge(sales_per_tile, on='TILE_NAME', how='left').fillna(0)\n",
    "osgb_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c309ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "osgb_tiles.plot(column='sales_count', ax=ax, legend=True, cmap='OrRd', legend_kwds={'label': \"Number of Sales\"})\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Number of Sales per 10km Grid Square')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average sale price per grid square\n",
    "avg_price_per_tile = gdf_sales_with_tiles.groupby('TILE_NAME')['price'].mean().reset_index(name = 'avg_price')\n",
    "avg_price_per_tile.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f98ff",
   "metadata": {},
   "source": [
    "7. Plot a map showing the average sale price per 10km grid square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge average price with OSGB tiles\n",
    "osgb_tiles = osgb_tiles.merge(avg_price_per_tile, on='TILE_NAME', how='left').fillna(0)\n",
    "\n",
    "# Plot the map (replace 'sales_count' with 'avg_price')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "osgb_tiles.plot(column='avg_price', ax=ax, legend=True, cmap='plasma') \n",
    "plt.title('Average Sale Price per 10km Grid Square')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340c984",
   "metadata": {},
   "source": [
    "8. Comment on your findings, \n",
    "Given everything you have learned from the exercises above, discuss the following model.\n",
    "In your discussion, provide an approximate R square value that you would expect from the\n",
    "model\n",
    "How would you improve the model?\n",
    "What range of R square would you be happy with?\n",
    "\n",
    "Price ~ β ∗ Y ear + ∑i=1 α +\n",
    "nα\n",
    "i ∑i=1 γ +\n",
    "nγ\n",
    "i ε\n",
    "\n",
    "Where: β\n",
    "is the coefficient for the year of sale αi and γi are dummy variables corresponding to postcode area and property type\n",
    "respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26643863",
   "metadata": {},
   "source": [
    "The current model estimates the relationship between sale price and year, while also accounting for categorical variables such as postcode area and property type. With these categorical variables in play, an R-squared value between 0.6 and 0.8 is considered reasonable, reflecting a moderately good fit depending on the complexity of the data. To enhance the model's performance, several improvements can be made. First, it is beneficial to incorporate additional features that could significantly influence sale prices, such as property size, number of bedrooms and bathrooms, lot size, property age, and condition or renovation status. Including these features can provide a more comprehensive view of what drives property values. \n",
    "\n",
    "Second, exploring interaction terms between features, like the interplay between year and property type or postcode area and property type, may reveal more nuanced relationships and improve model accuracy. Advanced modeling techniques such as random forests or gradient boosting machines could be employed if the data exhibits complex, non-linear relationships. These methods are adept at capturing intricate patterns without requiring explicit specification of interactions. \n",
    "\n",
    "Additionally, regularization techniques like Lasso or Ridge regression could be useful in managing multicollinearity and mitigating overfitting. While aiming for a higher R-squared is desirable, an acceptable R-squared range should be considered within the context of the data and the model's purpose. In real estate, where numerous factors impact property values, an R-squared in the 0.6 to 0.8 range might be quite acceptable, particularly if the model remains interpretable and avoids overfitting. Balancing model complexity with predictive accuracy and interpretability is key to achieving a robust and useful model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c82994",
   "metadata": {},
   "source": [
    "9. Energy Performance Certificates (EPCs) are published for each property transaction\n",
    "recorded by the Land Registry. The data schema for these is available here\n",
    "\n",
    "I. Which fields from this dataset would be useful in determining sale price?\n",
    "II. How would you approach the task of joining the EPC database with the Land Registry?\n",
    "N.B. We don't expect you to actually join the databases - but please provide as\n",
    "much detail as possible about how you would approach the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb84c07",
   "metadata": {},
   "source": [
    "To analyze the relationship between property sale prices and energy efficiency as recorded in Energy Performance Certificates (EPCs), it is useful to leverage specific fields from EPCs that are potentially influential in determining sale prices. The EPC data schema provides several pertinent fields. For instance, the **Energy Rating** is a key field as it indicates the property's energy efficiency; properties with higher ratings (e.g., A or B) might command higher sale prices due to anticipated savings on energy costs. Similarly, the **Property Type** field is valuable if EPC data categorizes properties in a way that aligns with the Land Registry’s classification. This alignment allows for meaningful comparisons of energy efficiency within property types. Additionally, **Floor Area** is crucial because the size of the property can affect both its energy efficiency and its market value. Larger properties with high energy ratings might fetch higher prices compared to smaller ones with the same rating, impacting the sale price on a price-per-unit-area basis. The **Main Heating Fuel** field indicates the type of fuel used for heating, with renewable or more efficient fuels potentially making the property more attractive to buyers and thus affecting its sale price. Finally, the **Age of Boiler** could be relevant as newer, more efficient boilers generally lower running costs, potentially influencing the sale price positively.\n",
    "\n",
    "To integrate and analyze data from both Land Registry and EPC databases, start by identifying a common unique identifier that links property transactions across the datasets. This identifier could be a property reference number, address, or a combination of fields. If the datasets are housed in different systems, a Database Management System (DBMS) or tools like SQL can be used to execute a join operation based on this identifier. As my preference, if the datasets are in CSV or Excel format, I will use programming languages Python, with libraries like Pandas, to facilitate the merging process.\n",
    "\n",
    "During the merging process, it is essential to address potential issues such as duplicate entries or missing values in the common identifier field. Techniques such as deduplication or imputation should be employed to handle these inconsistencies. Data cleaning is crucial to ensure consistency across datasets, including standardizing formats for fields like address or property type to achieve a more accurate join.\n",
    "\n",
    "Once the datasets are successfully merged, the combined dataset can be analyzed to explore how EPC features influence sale prices. Analytical techniques such as correlation analysis or regression modeling can be used to quantify the relationships between EPC features and property sale prices, revealing how energy efficiency might impact market values. This comprehensive approach provides insights into the economic implications of energy efficiency on property sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe5826",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
